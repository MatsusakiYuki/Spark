{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MD7WZdV5dKNl"
   },
   "source": [
  
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eQAl2EFIdudQ"
   },
   "source": [
    "### Step 1: Import pyspark and initialize Spark\n",
    " Create a SparkContext Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 51486,
     "status": "ok",
     "timestamp": 1566395300287,
     "user": {
      "displayName": "Anni Li",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mArP0D4dW8XVwVqb1Z_2ClQ75CFaAeOQQHaP2bE=s64",
      "userId": "00745771266953846278"
     },
     "user_tz": -600
    },
    "id": "P8ohkOGGPekF",
    "outputId": "d601ca18-c9ff-49f3-f92f-0d5dd5557f04"
   },
   "outputs": [],
   "source": [
    "#!pip install pyspark\n",
    "# create entry points to spark\n",
    "from pyspark import SparkContext, SparkConf # Spark\n",
    "from pyspark.sql import SparkSession # Spark SQL\n",
    "\n",
    "sc_conf = SparkConf().setMaster(\"local[*]\").setAppName('my spark app')\n",
    "\n",
    "sc = SparkContext.getOrCreate(conf=sc_conf)\n",
    "\n",
    "spark = SparkSession(sparkContext=sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q0hceZC5dZG7"
   },
   "source": [
    "### Step 2: Create RDD to read required files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bBd63nxnboGI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of lines in Book 1 is 21569\n"
     ]
    }
   ],
   "source": [
    "# read in Book1\n",
    "file1 = sc.textFile('Agile Processes  in Software Engineering  and Extreme Programming.txt')\n",
    "\n",
    "# display total numbers in each dataset\n",
    "display1 = file1.count()\n",
    "print(\"The total number of lines in Book 1 is \" + str(display1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of lines in Book 2 is 4617\n"
     ]
    }
   ],
   "source": [
    "# read in Book2\n",
    "file2 = sc.textFile('Scrum Handbook.txt')\n",
    "# display total numbers in each dataset\n",
    "display2 = file2.count()\n",
    "print(\"The total number of lines in Book 2 is \" + str(display2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 03: Cleaning/Manipulating text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lnbip',\n",
       " 'i helen sharp',\n",
       " 'tracy hall  eds',\n",
       " 'agile processes',\n",
       " 'in software engineering',\n",
       " 'and extreme programming',\n",
       " 'th international conference  xp',\n",
       " 'edinburgh  uk  may',\n",
       " 'proceedings',\n",
       " 'springer open']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import regular expression\n",
    "import re\n",
    "\n",
    "# Remove all non-alphabets for book1\n",
    "# Changing all upper case letters to lowercase\n",
    "# Strip the space at the beginning and end for sentences\n",
    "# Remove empty line generated in previous steps\n",
    "\n",
    "lower_file1 = file1.map(lambda line:re.sub(r'[^a-zA-Z\\s]',\" \",line))\\\n",
    "            .map(lambda line:line.lower())\\\n",
    "            .map(lambda line:line.strip())\\\n",
    "            .filter(lambda line:len(line) != 0)\n",
    "\n",
    "lower_file1.take(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jeff sutherlands',\n",
       " 'scrum handbook',\n",
       " 'everything',\n",
       " 'you need',\n",
       " 'to know',\n",
       " 'to start',\n",
       " 'a scrum project',\n",
       " 'in your',\n",
       " 'organization',\n",
       " 'scrum',\n",
       " 'training',\n",
       " 'institute',\n",
       " 'm w press',\n",
       " 'this book is dedicated to nobel laureate muhammad yunus and the',\n",
       " 'grameen bank for originating microenterprise development and the',\n",
       " 'accion international presidents advisory board responsible for much of',\n",
       " 'microenterprise development in the western hemisphere',\n",
       " 'the strategy for bootstrapping the poor out of poverty has been',\n",
       " 'a model for freeing hundreds of thousands of software developers from',\n",
       " 'developer abuse caused by poor management practices']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove all non-alphabets for book2\n",
    "# Changing all upper case letters to lowercase\n",
    "# Strip the space at the beginning and end for sentences\n",
    "# Remove empty line generated in previous steps\n",
    "\n",
    "lower_file2 = file2.map(lambda line:re.sub(r'[^a-zA-Z\\s]',\"\",line))\\\n",
    "            .map(lambda line:line.lower())\\\n",
    "            .map(lambda line:line.strip())\\\n",
    "            .filter(lambda line:len(line) != 0)\n",
    "\n",
    "lower_file2.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 Transforming the Data/Counting the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 8161),\n",
       " ('and', 3975),\n",
       " ('of', 3954),\n",
       " ('to', 3751),\n",
       " ('in', 3101),\n",
       " ('a', 2755),\n",
       " ('is', 1541),\n",
       " ('that', 1356),\n",
       " ('for', 1195),\n",
       " ('on', 1027),\n",
       " ('as', 1023),\n",
       " ('we', 980),\n",
       " ('with', 970),\n",
       " ('software', 931),\n",
       " ('this', 915),\n",
       " ('are', 785),\n",
       " ('agile', 784),\n",
       " ('it', 775),\n",
       " ('development', 748),\n",
       " ('was', 711)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split the context based on space for book 1\n",
    "words1 = lower_file1.flatMap(lambda x: x.split())\n",
    "\n",
    "# count the total number of appearance for each word\n",
    "# Sort the data in descending order\n",
    "counter1 = words1.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\\\n",
    "        .sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "\n",
    "counter1.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1238),\n",
       " ('of', 537),\n",
       " ('and', 534),\n",
       " ('to', 477),\n",
       " ('a', 451),\n",
       " ('scrum', 395),\n",
       " ('in', 360),\n",
       " ('is', 348),\n",
       " ('team', 260),\n",
       " ('product', 232),\n",
       " ('for', 195),\n",
       " ('that', 181),\n",
       " ('it', 165),\n",
       " ('on', 149),\n",
       " ('sprint', 146),\n",
       " ('this', 142),\n",
       " ('with', 132),\n",
       " ('as', 124),\n",
       " ('are', 119),\n",
       " ('by', 118)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split the context based on space for book 2\n",
    "words2 = lower_file2.flatMap(lambda x: x.split()) \n",
    "\n",
    "# count the total number of appearance for each word\n",
    "# Sort the data in descending order\n",
    "counter2 = words2.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\\\n",
    "        .sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "counter2.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8962 unique words in Book 1.\n"
     ]
    }
   ],
   "source": [
    "# find stopwords in nltk package\n",
    "# remove stop words for book1\n",
    "word_filter1 = counter1.filter(lambda word: word[0] not in stopwords.words('english'))\n",
    "\n",
    "print(\"There are \" + str(word_filter1.count()) + \" unique words in Book 1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words for book2\n",
    "word_filter2 = counter2.filter(lambda word: word[0] not in stopwords.words('english'))\n",
    "\n",
    "\n",
    "print(\"There are \" + str(word_filter2.count()) + \" unique words in Book 2.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Find the average occurrence of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add up the value for total occurence of unique words from Step5\n",
    "count_unique1 = word_filter1.map(lambda x : x[1]).sum()\n",
    "# find the total number of unique words\n",
    "print(count_unique1)\n",
    "word_occur1 = word_filter1.map(lambda x : x[0]).count()\n",
    "print(word_occur1)\n",
    "\n",
    "# average occurence for each unique word in Book 1\n",
    "avg_uniq1 = count_unique1/word_occur1\n",
    "print(\"The average occurence of the words in Book 1 is \" + str(round(avg_uniq1,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add up the value for total occurence of unique words from Step5\n",
    "count_unique2 = word_filter2.map(lambda x : x[1]).sum()\n",
    "# find the total number of unique words\n",
    "word_occur2 = word_filter2.map(lambda x : x[0]).count()\n",
    "\n",
    "# average occurence for each unique word in Book 2\n",
    "avg_uniq2 = count_unique2/word_occur2\n",
    "print(\"The average occurence of the words in Book 2 is \" + str(round(avg_uniq2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1 Compare the distribution of words in Book1 and Book2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for book 1\n",
    "occur_list1 = np.log10(word_filter1.map(lambda x: x[1]).collect())\n",
    "\n",
    "# prepare data for book 2\n",
    "occur_list2 = np.log10(word_filter2.map(lambda x: x[1]).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot for word distribution\n",
    "fig = plt.gcf()\n",
    "plt.subplot(211)\n",
    "plt.hist(occur_list1, label=\"Book1\", color='#50c7c7')\n",
    "plt.xlabel('unique words', fontsize=15)       # add label to x axis\n",
    "plt.ylabel('count of unique words', fontsize=15)   # add label to y axis\n",
    "plt.title('Distribution of words in Book1', fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.show()\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.hist(occur_list2, label=\"Book2\", color='#50c7c7')\n",
    "plt.xlabel('unique words', fontsize=15)       # add label to x axis\n",
    "plt.ylabel('count of unique words', fontsize=15)   # add label to y axis\n",
    "plt.title('Distribution of words in Book2', fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.show() \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Book 1, the maximum count of unique words reaches ranges from over 3000 to 0. In contrast, the the maximum count of unique words in Book 2 ranges from around 1500 to 0. \n",
    "\n",
    "For both charts, it can beseen that the first few unique words take over 80 percent of the occurence of the total occurence count. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2 Compare the top 15 most common words in Book1 and Book2.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for Book 1 top words\n",
    "\n",
    "word_toplist1 = word_filter1.map(lambda x: x[0]).take(15)\n",
    "count_toplist1 = np.log10(word_filter1.map(lambda x: x[1]).take(15))\n",
    "\n",
    "# prepare data for Book 2 top words\n",
    "word_toplist2 = word_filter2.map(lambda x: x[0]).take(15)\n",
    "count_toplist2 = np.log10(word_filter2.map(lambda x: x[1]).take(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot figure\n",
    "fig = plt.gcf()\n",
    "plt.subplot(211)\n",
    "plt.ylabel('top unique words', fontsize=15)       # add label to x axis\n",
    "plt.xlabel('count of top unique words', fontsize=15)   # add label to y axis\n",
    "plt.bar(word_toplist1,count_toplist1)\n",
    "plt.title('Top 15 unique words in Book 1', fontsize=15)\n",
    "plt.xticks(fontsize=12, rotation=45, wrap=True)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show() \n",
    "\n",
    "plt.subplot(212)\n",
    "plt.ylabel('top unique words')       # add label to x axis\n",
    "plt.xlabel('count of top unique words', fontsize=15)   # add label to y axis\n",
    "plt.bar(word_toplist2, count_toplist2)\n",
    "plt.title('Top 15 unique words in Book 2', fontsize=15)\n",
    "plt.xticks(fontsize=12, rotation=45, wrap=True)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 15 unique words in the two books are sorted in descending order, as shown in the figures above. \n",
    "\n",
    "In Book 1, the first top word is \"software\" with count of about 3. This word is also incuded in the title of Book 1. Similarly, the word \"agile\" appeared in the title of Book 1 takes the second place in the top words. The count of \"agile\" is very close to \"software\". They are also the key words in the title of Book 1. Additionally, among the top 15 words, 12 of the words are about software developing and team work, whereas the rest three(one, time, also) are commonly used sight words.Therefore, it can be seen that Book 1 is closely related to software development.\n",
    " \n",
    "Book 2 has the first top word of scrum, which is also the first word in the title of the book. The first few top words includes \"team\", \"product\", \"development\", \"teams\", \"project\", which are also part of top words in Book 1. Although the top 15 words also contain \"software\", the count is less than that in Book 1. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment 1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
